[{"path":"/articles/breakpt-ex.html","id":"change-points-in-a-time-course","dir":"Articles","previous_headings":"","what":"Change points in a time course","title":"Breakpoint example","text":"order use Sys.sleep() actual computations, chose toy problem inference change points time course repeated observations measurement. First, start simulating data increasing integer x coordinate (time point) continuous y coordinate (observation value). use mcp package detect point x value y changes abruptly. package able infer many different kinds change points. , make use changing intercepts constant, normally distributed noise. package built rjags, uses Gibbs sampling Markov-Chain Monte Carlo (MCMC). computations usually computationally expensive, individual sampling chains independent . makes kind problem amenable extensive parallelization. Fortunately, rjags package already provided module, can load using following command: running , available within R via library(rjags). still need install.packages(\"mcp\"), require external tools work without issues. Note system-wide R package library writable individual users, use user library home directory (R prompt ).","code":"module load rjags/4-10-R-4.1.2"},{"path":"/articles/breakpt-ex.html","id":"creating-a-function-to-simulate-data","dir":"Articles","previous_headings":"","what":"Creating a function to simulate data","title":"Breakpoint example","text":"can use following function simulate data: randomly generate one sample x y coordinates. resulting data look something like :","code":"simulate_data = function(n=1, len=1000, bps=c(0,1,2)) {     # simulate one data set with random break points and y coordinates     simulate_one = function() {         sim = data.frame(x=seq_len(len), y=rnorm(len))         for (bp in sample(bps, 1)) {             loc = sample(seq_len(len), 1)             sim$y[loc:len] = sim$y[loc:len] + rnorm(1)         }         sim     }      # if we simulate one data set, return a data.frame, otherwise a list thereof     res = replicate(n, simulate_one(), simplify=FALSE)     if (length(res) == 1) {         res[[1]]     } else {         res     } } sim = simulate_data(bps=1) plot(sim)"},{"path":"/articles/breakpt-ex.html","id":"running-break-point-inference","dir":"Articles","previous_headings":"","what":"Running break point inference","title":"Breakpoint example","text":"can now use simulated data starting point infer breakpoints simulated. Using mcp package, run three different models: One segment, breakpoints Two segments, one breakpoint Three segments, two breakpoints compare models terms well fit data, return model fits best: resulting model look something like :  , see x y coordinates observations , addition see traces intercepts (grey lines) probability density break point (blue line close bottom plot).","code":"library(mcp)  run_mcp = function(sim) {     # one unbroken segment, 2 segments with one break point, 3 segments 2 bps     mods = list(list(y ~ 1),                 list(y ~ 1, ~ 1),                 list(y ~ 1, ~ 1, ~ 1))      # fit all three models, select the best using leave-one-out     fits = lapply(mods, function(m) mcp::mcp(m, data=sim, par_x=\"x\"))     compare = as.data.frame(loo::loo_compare(lapply(fits, mcp::loo)))     best = as.integer(sub(\"model\", \"\", rownames(compare)))[1]     fits[[best]] } mod = run_mcp(sim) #> Compiling model graph #>    Resolving undeclared variables #>    Allocating nodes #> Graph information: #>    Observed stochastic nodes: 1000 #>    Unobserved stochastic nodes: 2 #>    Total graph size: 5019 #>  #> Initializing model #> Finished sampling in 18.9 seconds #> Compiling model graph #>    Resolving undeclared variables #>    Allocating nodes #> Graph information: #>    Observed stochastic nodes: 1000 #>    Unobserved stochastic nodes: 4 #>    Total graph size: 12020 #>  #> Initializing model #> Finished sampling in 72.5 seconds #> Compiling model graph #>    Resolving undeclared variables #>    Allocating nodes #> Graph information: #>    Observed stochastic nodes: 1000 #>    Unobserved stochastic nodes: 6 #>    Total graph size: 17028 #>  #> Initializing model #> Finished sampling in 147.3 seconds #> Warning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details.  #> Warning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details. plot(mod)"},{"path":"/articles/breakpt-ex.html","id":"exercise","dir":"Articles","previous_headings":"Running break point inference","what":"Exercise","title":"Breakpoint example","text":"Use editor create break point simulation script (hint: can use Esc+:set paste nvim copy-paste text without automatic indentation :set nopaste return) Simulate data one two break points Start interactive job 1 task 5 cores Run mcp inference script try estimate breakpoints data. match parameters simulation? (hint: can use plot function mcp model object; create Rplots.pdf ’ll need dev.() copy local machine) options see make code run faster? (hint: mcp-provided parallelism sequential steps inference code) much runtime can save running computations parallel?","code":""},{"path":"/articles/breakpt-ex.html","id":"bigger-data-sets","dir":"Articles","previous_headings":"","what":"Bigger data sets","title":"Breakpoint example","text":"strategy reserving node many CPUs works well extent want process computations parallel CPUs given node (modern nodes often 128 cores/threads). , want simulate big computational task, use many resources limit overall amount.","code":""},{"path":"/articles/breakpt-ex.html","id":"exercise-1","dir":"Articles","previous_headings":"Bigger data sets","what":"Exercise","title":"Breakpoint example","text":"Simulate 10 breakpoint data sets using function , save resulting list .rds object (using saveRDS) Write submission script 10 tasks load object, subset current task index, save resulting model .rds model plot .pdf (hint: automatic environment variables available slurm run, SLURM_PROCID)","code":""},{"path":"/articles/breakpt-ex.html","id":"hpc-specific-packages","dir":"Articles","previous_headings":"","what":"HPC-specific packages","title":"Breakpoint example","text":"also multiple packages available make use HPC resources within R. say, R submit job multiple jobs, retrieve result back session. , instance, packages BatchJobs batchtools. make use networked file system write call arguments file, retrieved job excecuted. packages robust small numbers jobs, however, put substantial strain file system high number function calls. Instead, introduce two packages make little use shared file system, slurmR clustermq. (Note author latter, ’s bit conflict interest .)","code":""},{"path":"/articles/breakpt-ex.html","id":"slurmr","dir":"Articles","previous_headings":"HPC-specific packages","what":"slurmR","title":"Breakpoint example","text":"slurmR package lightweight (dependency-free) R package allows users interact scheduler. can used create cluster object Slurm tasks analogous parallel::makePSOCKcluster. command slurmR::makeSlurmCluster(ntasks), can used parLapply parSapply functions: sbatch call created via makeSlurmCluster can customized passing different parameters cluster creation function. particular, need : Include right budgeting account details can found Getting Started vignette.","code":"library(slurmR) cl = makeSlurmCluster(5)  res =  parSapply(1:100, function(x) mean(runif(100)))  stopCluster(cl)"},{"path":"/articles/breakpt-ex.html","id":"clustermq","dir":"Articles","previous_headings":"HPC-specific packages","what":"clustermq","title":"Breakpoint example","text":"clustermq package provides interface multiple HPC schedulers (including Slurm) via ZeroMQ socket library, available module: package relies submission template, default submits Slurm jobs job array. need supply account name, policy Sulis use tasks whenever possible, modify submission template : Include right budgeting account Load right modules submission script Use srun multiple tasks instead job array new template look something like : can use newly created template : make example run. information available User Guide vignette.","code":"module load GCCcore/10.2.0 ZeroMQ/4.3.3 library(clustermq)  fx = function(x) x * 2  Q(fx, x=1:3, n_jobs=1) #!/bin/sh #SBATCH --account={{ account | su105 }} #SBATCH --job-name={{ job_name }} #SBATCH --tasks={{ n_jobs }} #SBATCH --mem-per-cpu={{ memory | 1024M }} #SBATCH --cpus-per-task={{ cores | 1 }}  CMQ_AUTH={{ auth }} srun R --no-save --no-restore -e 'clustermq:::worker(\"{{ master }}\")' options(clustermq.template = \"/path/to/updated/template\")"},{"path":"/articles/breakpt-ex.html","id":"exercise-2","dir":"Articles","previous_headings":"HPC-specific packages","what":"Exercise","title":"Breakpoint example","text":"Run workers slurmR Run workers clustermq","code":""},{"path":[]},{"path":[]},{"path":"/articles/quickstart.html","id":"prerequesites","dir":"Articles","previous_headings":"","what":"Prerequesites","title":"Quick Start","text":"Working High-Performance Computing (HPC) facilities, primarily interface systems via command-line shell. expect people course wide range expertise different starting points concerning use command-line, make easier challenging participants, depending coming . Similarly, expect participants start different operating systems, e.g. Windows also macOS different Linux flavors. order keep course consistent recommend Windows users work materials using Windows Subsystem Linux (WSL 2.0). available --date Windows 10 later. expect macOS Linux users already familiar terminal.","code":""},{"path":"/articles/quickstart.html","id":"connecting-to-the-computing-cluster","dir":"Articles","previous_headings":"","what":"Connecting to the computing cluster","title":"Quick Start","text":"Command-line connections HPC established using Secure Shell tool (SSH). terminal, can connect Sulis using following line: Since bit tedious, can outsource information ~/.ssh/config file following contents: set like , establishing connection simple typing: come handy connecting HPC , also copying files local machine HPC. Note, however, cases still need decrypt private key file passphrase (, least macOS Linux can automated using OS keychain) 2FA token day.","code":"ssh -i <your keyfile> <user>@login.sulis.ac.uk Host sulis     Hostname login.sulis.ac.uk     User <username> #    ProxyCommand ssh <proxy> -W %h:%p   # when connecting via an SSH proxy     IdentityFile ~/.ssh/<sulis_rsa> ssh sulis"},{"path":"/articles/quickstart.html","id":"setting-up-r","dir":"Articles","previous_headings":"","what":"Setting up R","title":"Quick Start","text":"","code":"which R # /usr/bin/which: no R in [...] module load GCC/11.2.0 OpenMPI/4.1.1 R/4.1.2 which R # /sulis/easybuild/software/R/4.1.2-foss-2021b/bin/R"},{"path":"/articles/quickstart.html","id":"running-an-interactive-job","dir":"Articles","previous_headings":"","what":"Running an interactive job","title":"Quick Start","text":"simplest way request interactive job use Slurm’s srun command specify want run shell (specified $SHELL environment variable) connected terminal input output (--pty). addition, need specify account requested resources budgeted (--account) can running: see command prompt changes user@login user$nodeXX, means now connected compute node instead login node. , allowed run heavy computations within resource constraints specified. First, let’s get overview processes already running node. can running resource monitor htop: overview, can see many cores compute node , many processes running, much memory used. Depending much resources requested (overall load), see least resource allocation still free. , however, need stay within allocation (overall amount available resources), otherwise processes terminated automatically. loaded R module beforehand, ’ll see $PATH (environment variable shell looks executables) still set include R. can check asking R path: can run via command-line, can local machines well. Running R give us R command prompt: can use R use R shell e.g. RStudio well:","code":"srun --account su105 --pty $SHELL htop which R #>  #> R version 4.1.2 (2021-11-01) -- \"Bird Hippie\" #> Copyright (C) 2021 The R Foundation for Statistical Computing #> Platform: x86_64-pc-linux-gnu (64-bit) #>  #> R is free software and comes with ABSOLUTELY NO WARRANTY. #> You are welcome to redistribute it under certain conditions. #> Type 'license()' or 'licence()' for distribution details. #>  #>   Natural language support but running in an English locale #>  #> R is a collaborative project with many contributors. #> Type 'contributors()' for more information and #> 'citation()' on how to cite R or R packages in publications. #>  #> Type 'demo()' for some demos, 'help()' for on-line help, or #> 'help.start()' for an HTML browser interface to help. #> Type 'q()' to quit R. #>  #> > quit() x = 5 y = 3 x * y #> [1] 15"},{"path":"/articles/quickstart.html","id":"copying-files","dir":"Articles","previous_headings":"","what":"Copying files","title":"Quick Start","text":"different options getting files compute cluster. One option edit files locally, copy SSH. , instance, one local file text.txt copy : specified host ~/.ssh/config , otherwise may need specify user name, key file, host manually (look differently using graphical SSH client). : used scp command know one remote end, .e. use following copy file remote local end: , scp command looks test.sh home directory (~; default directory specified) copy current directory (.). want copy directories need use recursive copying, .e. scp -r. Another, maybe better alternative rsync command. keep timestamps intact, can used copy files updated timestamps (-u) compared local files (-v print files copying):","code":"scp text.txt host: scp host:test.txt . rsync -uvr host:test.txt ."},{"path":"/articles/quickstart.html","id":"editing-files","dir":"Articles","previous_headings":"","what":"Editing files","title":"Quick Start","text":"either making small changes iterative work, often convenient edit files directly computing cluster instead editing locally copy . multiple text-based editors work terminal, nano, emacs vim. nano minimalist editor without special features, often recommended users new terminal. problem get quickly stuck local optimum, can make simple changes file, never get features syntax highlighting. two editors hand either can extended /every feature imaginable. course, show basic features nvim (neovim, modern implementation vim) instead. already user emacs, please feel free use editor instead. edit simple text file, can run: see console gets cleared, shown contents empty file instead. Try typing couple file: see characters show , another text editor well. Notice, however, first typed show , subsequent . editor “normal” “edit” mode. typing first , switched first latter. can now use Esc switch back edit normal mode. normal mode, can type :w Enter write file, :wq write file quit editor. :q! exit without saving changes. need know make vim useful nano. However, now instance edit .r file also get syntax highlighting. feature alone makes worth use nvim instead nano. can explore features running vimtutor command-line, interactive tool familiarize use effectively.","code":"nvim myfile.txt aaaa"},{"path":"/articles/quickstart.html","id":"compute-resources","dir":"Articles","previous_headings":"","what":"Compute resources","title":"Quick Start","text":"now, submitted job specifying minimum required parameters relying defaults others. instance, specific partition, one several job queues can submit jobs . get overview available, can use sinfo command: , see different partitions listed number nodes associated , including walltime (maximum amount time job can request) queue. see one queue marked *. denotes default queue, using specifying particular queue via --partition parameter. One argument specify explain detail account. specifies connection user name collection resources available can use, subtracted budget. can check accounts user access typing: likely belong one account, project, time (one created course).","code":"sinfo sacctmgr show associations where user=<your user name>"},{"path":"/articles/quickstart.html","id":"job-submission-scripts","dir":"Articles","previous_headings":"","what":"Job submission scripts","title":"Quick Start","text":"Usually, want run complex computations can specified single srun. running multiple commands multiple hosts, often better specify resource requirements exact commands using job submission script. may look like following: can submit script saving script file running sbatch <script>. tell us something like: Submitted batch job 1290046 number identifier job (different multiple runs). started , can check jobs user running typing: list job ID, show currently running, node job running . can also get detailed information job using scontrol: information available job running (short) time finished. can now also see resources explicitly request, e.g. time limit 1 hour, used compute partition, able use 2 Gb memory. job started, create output file called slurm-xxxxxx.(xxxxxx job id) standard output command (standard output otherwise printed console). finished, contain output uname -n, name node command run . went well, contain nodeXX login, means run one compute nodes. ’s quite lines , let’s break : #!/bin/sh called shebang specifies application used run script, required sbatch (otherwise refuse submit job) #SBATCH --account needed budget resources correctly #SBATCH --parititon time specific compute partition explicitly #SBATCH --ntasks lists tasks (computations) job contains #SBATCH --cpus-per-task specifies numbers CPUs per task #SBATCH --mem specifies amount memory requested; total, options include --mem-per-task, --mem-per-cpu, --mem-per-gpu. Memory multipliers K, M G supported (kilobytes, megabytes gigabytes, respectively). #SBATCH --time maximum amount time job terminated (dd-hh:mm:ss) #SBATCH commands need directly following shebang, otherwise ignored srun specifies command run; required running individual computations, helps set parallel helpers, running call per task, e.g. setting MPI used","code":"#!/bin/sh #SBATCH --account su105 #SBATCH --partition compute #SBATCH --ntasks 1 #SBATCH --cpus-per-task 1 #SBATCH --mem 1024M #SBATCH --time 8:00:00  srun uname -n squeue -u <username> scontrol show jobid <jobid>"},{"path":"/articles/quickstart.html","id":"exercise","dir":"Articles","previous_headings":"Job submission scripts","what":"Exercise","title":"Quick Start","text":"Create batch submission script like one using command-line editor cluster Submit using sbatch <script>, without srun. changes? happens change ntasks parameter 2, without srun command?","code":""},{"path":"/articles/quickstart.html","id":"r-on-hpc","dir":"Articles","previous_headings":"","what":"R on HPC","title":"Quick Start","text":"R’s best developed parallel capabilities running computations multiple cores. , briefly outline approaches commonly used. simplicity, let’s consider simple R function sleeps couple seconds: need call function 10 times, use something like lapply: , unsurprisingly, take 10 times long individual call fsleep(). many use cases, makese sense run computations (done reality instead just sleeping) parallel. probably integrated solution parallel package, one core packages distributed standard R installation default. However, also possibilities, e.g. foreach package enables parallel processing using %dopar% command, future package using plan(multisession).","code":"fsleep = function(i) {     print(\"starting \", i)     Sys.sleep(1)     print(\"done!\") }  fsleep(1) #> [1] \"starting \" #> [1] \"done!\" system.time({ lapply(1:10, fsleep) }) #> [1] \"starting \" #> [1] \"done!\" #> [1] \"starting \" #> [1] \"done!\" #> [1] \"starting \" #> [1] \"done!\" #> [1] \"starting \" #> [1] \"done!\" #> [1] \"starting \" #> [1] \"done!\" #> [1] \"starting \" #> [1] \"done!\" #> [1] \"starting \" #> [1] \"done!\" #> [1] \"starting \" #> [1] \"done!\" #> [1] \"starting \" #> [1] \"done!\" #> [1] \"starting \" #> [1] \"done!\" #>    user  system elapsed  #>   0.004   0.000  10.015 system.time({ parallel::mclapply(1:10, fsleep) }) #>    user  system elapsed  #>   0.014   0.000   5.020"},{"path":"/articles/quickstart.html","id":"exercise-1","dir":"Articles","previous_headings":"R on HPC","what":"Exercise","title":"Quick Start","text":"Create “sleep script” computing cluster submission script request 1 task 5 cores. Submit script job. long take? runtime make sense? Submit script using parallel foreach loop PSOCK cluster. run? results make sense? newer approach aims providing common interface many parallel backends future package. Can run example using plan(multisession) future.apply?","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":[]},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Michael Schubert. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Schubert M (2022). Rhpc: R-hpc Course Material. R package version 0.0.1, https://mschubert.github.io/R-hpc/.","code":"@Manual{,   title = {Rhpc: R-hpc Course Material},   author = {Michael Schubert},   year = {2022},   note = {R package version 0.0.1},   url = {https://mschubert.github.io/R-hpc/}, }"},{"path":"/index.html","id":"high-performance-computing-with-r","dir":"","previous_headings":"","what":"R-hpc Course Material","title":"R-hpc Course Material","text":"brief overview course content.","code":""},{"path":"/index.html","id":"day-1-morning-technical-introduction-by-hpc-admins","dir":"","previous_headings":"","what":"Day 1 morning: Technical Introduction by HPC admins","title":"R-hpc Course Material","text":"Capabilities computing cluster connect Starting basic job","code":""},{"path":"/index.html","id":"day-1-afternoon-quick-start","dir":"","previous_headings":"","what":"Day 1 afternoon: Quick Start","title":"R-hpc Course Material","text":"Copying editing files via command-line Interactive jobs, batch jobs parallel package, cluster objects, future","code":""},{"path":"/index.html","id":"day-1-afternoon-neovim-as-ide","dir":"","previous_headings":"","what":"Day 1 afternoon: Neovim as IDE","title":"R-hpc Course Material","text":"Nvim-R plugin interactively develop remote session Persistent server sessions using tmux","code":""},{"path":"/index.html","id":"day-2-morning-breakpoint-example","dir":"","previous_headings":"","what":"Day 2 morning: Breakpoint example","title":"R-hpc Course Material","text":"MCMC breakpoint detection method example Tasks vs. threads bigger jobs R packages HPC use: slurmR clustermq","code":""},{"path":"/index.html","id":"day-2-afternoon-workflows","dir":"","previous_headings":"","what":"Day 2 afternoon: Workflows","title":"R-hpc Course Material","text":"(available yet)","code":""},{"path":"/index.html","id":"day-2-afternoon","dir":"","previous_headings":"","what":"Day 2 afternoon:","title":"R-hpc Course Material","text":"Possibility attendees work problems help instructors","code":""},{"path":[]}]
