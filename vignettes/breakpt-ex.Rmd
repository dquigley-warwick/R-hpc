---
title: "Breakpoint example"
output:
  rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Breakpoint example}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{css echo=FALSE}
img {
    border: 0px !important;
    margin: 2em 2em 2em 2em !important;
}
code {
    border: 0px !important;
}
kbd {
    background-color: #eee;
    border-radius: 3px;
    border: 1px solid #b4b4b4;
    box-shadow: 0 1px 1px rgba(0, 0, 0, .2), 0 2px 0 0 rgba(255, 255, 255, .7) inset;
    color: #333;
    display: inline-block;
    font-size: .85em;
    font-weight: 700;
    line-height: 1;
    padding: 2px 4px;
    white-space: nowrap;
}
```

```{r echo=FALSE, results="hide"}
knitr::opts_chunk$set(
    cache = FALSE,
    echo = TRUE,
    collapse = TRUE,
    comment = "#>"
)
```

## Change points in a time course

In order to not only use `Sys.sleep()` but do some actual computations, we chose
as a toy problem the inference of change points in a time course with repeated
observations of the same measurement. First, we will start off with simulating
data that has an increasing integer `x` coordinate (time point) and a continuous
`y` coordinate (observation value).

We will then use the `mcp` package to detect a point in `x` where the value of
`y` changes abruptly. This package is able to infer many different kinds of
change points. Here, we make use of changing intercepts with constant, normally
distributed noise. The package is built on `rjags`, which uses a Gibbs sampling
Markov-Chain Monte Carlo (MCMC).

These computations are usually computationally expensive, but the individual
sampling chains are independent of each other. This makes this kind of problem
amenable to extensive parallelization.

Fortunately, the `rjags` package is already provided as a module, so we can load
it using the following command:

```{sh eval=FALSE}
module load rjags/4-10-R-4.1.2
```

After running this, it should be available from within R via `library(rjags)`.
We still need to `install.packages("mcp")`, but as this does not require
external tools it should work without issues. Note that the system-wide R
package library is not writable for individual users, so you have to use your
user library in your home directory (R will prompt you to do this).

## Creating a function to simulate data

We can use the following function to simulate our data:

```{r}
simulate_data = function(n=1, len=1000, bps=c(0,1,2)) {
    # simulate one data set with random break points and y coordinates
    simulate_one = function() {
        sim = data.frame(x=seq_len(len), y=rnorm(len))
        for (bp in seq_len(sample(bps, 1))) {
            loc = sample(seq_len(len), 1)
            sim$y[loc:len] = sim$y[loc:len] + rnorm(1)
        }
        sim
    }

    # if we simulate one data set, return a data.frame, otherwise a list thereof
    res = replicate(n, simulate_one(), simplify=FALSE)
    if (length(res) == 1) {
        res[[1]]
    } else {
        res
    }
}
```

This will randomly generate one sample with our `x` and `y` coordinates. The
resulting data should look something like this:

```{r}
sim = simulate_data(bps=1)
plot(sim)
```

## Running break point inference

We can now use our simulated data as a starting point to infer the breakpoints
that we simulated. Using the `mcp` package, we run three different models:

* One segment, no breakpoints
* Two segments, one breakpoint
* Three segments, two breakpoints

When then compare these models in terms of how well they fit the data, and
return the model that fits best:

```{r}
library(mcp)

run_mcp = function(sim) {
    # one unbroken segment, 2 segments with one break point, 3 segments 2 bps
    mods = list(list(y ~ 1),
                list(y ~ 1, ~ 1),
                list(y ~ 1, ~ 1, ~ 1))

    # fit all three models, select the best using leave-one-out
    fits = lapply(mods, function(m) mcp::mcp(m, data=sim, par_x="x"))
    compare = as.data.frame(loo::loo_compare(lapply(fits, mcp::loo)))
    best = as.integer(sub("model", "", rownames(compare)))[1]
    fits[[best]]
}
```

The resulting model should look something like this:

```{r}
mod = run_mcp(sim)
plot(mod)
```

Here, we see the `x` and `y` coordinates of our observations as before, but in
addition we see the traces of the intercepts (grey lines) and the probability
density of the break point itself (blue line close to the bottom of the plot).

#### Exercise

* Use your editor to create the break point simulation script (_hint_: you can
  use <kbd>Esc</kbd>+`:set paste` in nvim to copy-paste the text without
  automatic indentation and `:set nopaste` to return)
* Simulate data for one and two break points
* Start an interactive job with 1 task and 5 cores
* Run the `mcp` inference script to try to estimate the breakpoints from the
  data. Do they match to the parameters of your simulation? (_hint_: you can use
  the `plot` function on a `mcp` model object; this will create a `Rplots.pdf`
  after which you'll need to `dev.off()` and copy to your local machine)
* Which options do you see to make this code run faster? (_hint_: there is both
  `mcp`-provided parallelism and sequential steps in the inference code)
* How much runtime can you save by running computations in parallel?

## Bigger data sets

The strategy of reserving a node with many CPUs works well up to the extent
where we want to process more computations in parallel than there are CPUs on a
given node (modern nodes often have up to 128 cores/threads).

Here, we want to simulate such a big computational task, but for this to not use
too many resources we will limit the overall amount.

#### Exercise

* Simulate 10 breakpoint data sets using the function above, and save the
  resulting list in an `.rds` object (using `saveRDS`)
* Write a submission script with 10 tasks that load the object, subset the
  current task index, and save the resulting model as `.rds` and model plot as
  `.pdf` (_hint_: there are automatic environment variables available for each
  Slurm run, such as [`SLURM_PROCID`](https://slurm.schedmd.com/sbatch.html#lbAK))

## HPC-specific packages

There are also multiple packages available to make use of HPC resources from
within R. That is to say, that R will submit a job or multiple jobs, and
retrieve the result back to the session.

There are, for instance, the packages `BatchJobs` and `batchtools`. These make
use of the networked file system to write each call and its arguments to a file,
that is then retrieved by the job and executed. These packages are robust for
small numbers of jobs, however, they will put a substantial strain on the file
system for a high number of function calls.

Instead, we will introduce two packages that make only little use of the shared
file system, `slurmR` and `clustermq`. (Note that I am the author of the latter,
so there's a bit of a conflict of interest here.)

#### slurmR

The [`slurmR`](https://uscbiostats.github.io/slurmR/) package is a lightweight
(dependency-free) R package that allows users to interact with the scheduler.

It can be used to create a cluster object on Slurm tasks analogous to
`parallel::makePSOCKcluster`. The command for this is
`slurmR::makeSlurmCluster(ntasks)`, which can then be used with the `parLapply`
or `parSapply` functions:

```{r eval=FALSE}
library(slurmR)

cl = makeSlurmCluster(5, account="su105")

res = parSapply(cl, 1:100, function(x) { Sys.sleep(1); mean(runif(100)) })

stopCluster(cl)
```

The `sbatch` call that is created via `makeSlurmCluster` can be customized by
passing different parameters to the cluster creation function. In particular, we
need to:

* Include the right budgeting account

More details can be found in their [Getting Started
vignette](https://uscbiostats.github.io/slurmR/articles/getting-started.html).

#### clustermq

The [`clustermq`](https://mschubert.github.io/clustermq/) package provides an
interface to multiple HPC schedulers (including Slurm) via the ZeroMQ socket
library, available as a module:

```{sh eval=FALSE}
module load GCCcore/10.2.0 ZeroMQ/4.3.3
```

```{r eval=FALSE}
library(clustermq)

fx = function(x) x * 2

Q(fx, x=1:3, n_jobs=1)
```

The package relies on a [submission
template](https://github.com/mschubert/clustermq/blob/master/inst/SLURM.tmpl),
which by default submits Slurm jobs as a job array. As we need to supply an
account name, and the policy on Sulis is to use tasks whenever possible, we
should modify the submission template to:

* Include the right budgeting account
* Load the right modules in the submission script
* Use `srun` with multiple tasks per index in the job array

The new template should look something like this:

```{sh eval=FALSE}
#!/bin/sh
#SBATCH --account={{ account | su105 }}
#SBATCH --job-name={{ job_name }}
#SBATCH --array=1-{{ n_jobs }}
#SBATCH --tasks={{ tasks | 20 }}
#SBATCH --mem-per-cpu={{ memory | 1024M }}
#SBATCH --cpus-per-task={{ cores | 1 }}

CMQ_AUTH={{ auth }} srun R --no-save --no-restore -e 'clustermq:::worker("{{ master }}")'
```

We can then use this newly created template by:

```{r eval=FALSE}
options(clustermq.host = "ib0", # faster network
        clustermq.template = "/path/to/updated/template")
```

Note that you can add these lines in your `~/.Rprofile` to set automatically.

This should make the above example run with 20 tasks per job requested. More
information on how to configure and use `clustermq` is available at the [User
Guide vignette](https://mschubert.github.io/clustermq/articles/userguide.html).

#### Exercise

* Run the above `mcp` example using one interactive "master" job (e.g. using the
  R integration with `nvim`) and
  * Run the workers with `slurmR`
  * Run the workers with `clustermq`
